{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3628463",
   "metadata": {},
   "source": [
    "# Ridge and Lassso Regression\n",
    "\n",
    "[Resource](https://harvard-iacs.github.io/2017-CS109A/lectures/lecture7/notebook/)\n",
    "\n",
    "Nice lil Harvard University resource here for ya. Who needs a masters! (Just kidding, I probably will at some point).\n",
    "\n",
    "Let's get these bajillion imports out of the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eddb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools import add_constant\n",
    "from statsmodels.regression.linear_model import RegressionResults\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b56ffa18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Return a regularized fit to a linear regression model.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "method : str\n",
      "    Either 'elastic_net' or 'sqrt_lasso'.\n",
      "alpha : scalar or array_like\n",
      "    The penalty weight.  If a scalar, the same penalty weight\n",
      "    applies to all variables in the model.  If a vector, it\n",
      "    must have the same length as `params`, and contains a\n",
      "    penalty weight for each coefficient.\n",
      "L1_wt : scalar\n",
      "    The fraction of the penalty given to the L1 penalty term.\n",
      "    Must be between 0 and 1 (inclusive).  If 0, the fit is a\n",
      "    ridge fit, if 1 it is a lasso fit.\n",
      "start_params : array_like\n",
      "    Starting values for ``params``.\n",
      "profile_scale : bool\n",
      "    If True the penalized fit is computed using the profile\n",
      "    (concentrated) log-likelihood for the Gaussian model.\n",
      "    Otherwise the fit uses the residual sum of squares.\n",
      "refit : bool\n",
      "    If True, the model is refit using only the variables that\n",
      "    have non-zero coefficients in the regularized fit.  The\n",
      "    refitted model is not regularized.\n",
      "**kwargs\n",
      "    Additional keyword arguments that contain information used when\n",
      "    constructing a model using the formula interface.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "statsmodels.base.elastic_net.RegularizedResults\n",
      "    The regularized results.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The elastic net uses a combination of L1 and L2 penalties.\n",
      "The implementation closely follows the glmnet package in R.\n",
      "\n",
      "The function that is minimized is:\n",
      "\n",
      ".. math::\n",
      "\n",
      "    0.5*RSS/n + alpha*((1-L1\\_wt)*|params|_2^2/2 + L1\\_wt*|params|_1)\n",
      "\n",
      "where RSS is the usual regression sum of squares, n is the\n",
      "sample size, and :math:`|*|_1` and :math:`|*|_2` are the L1 and L2\n",
      "norms.\n",
      "\n",
      "For WLS and GLS, the RSS is calculated using the whitened endog and\n",
      "exog data.\n",
      "\n",
      "Post-estimation results are based on the same data used to\n",
      "select variables, hence may be subject to overfitting biases.\n",
      "\n",
      "The elastic_net method uses the following keyword arguments:\n",
      "\n",
      "maxiter : int\n",
      "    Maximum number of iterations\n",
      "cnvrg_tol : float\n",
      "    Convergence threshold for line searches\n",
      "zero_tol : float\n",
      "    Coefficients below this threshold are treated as zero.\n",
      "\n",
      "The square root lasso approach is a variation of the Lasso\n",
      "that is largely self-tuning (the optimal tuning parameter\n",
      "does not depend on the standard deviation of the regression\n",
      "errors).  If the errors are Gaussian, the tuning parameter\n",
      "can be taken to be\n",
      "\n",
      "alpha = 1.1 * np.sqrt(n) * norm.ppf(1 - 0.05 / (2 * p))\n",
      "\n",
      "where n is the sample size and p is the number of predictors.\n",
      "\n",
      "The square root lasso uses the following keyword arguments:\n",
      "\n",
      "zero_tol : float\n",
      "    Coefficients below this threshold are treated as zero.\n",
      "\n",
      "The cvxopt module is required to estimate model using the square root\n",
      "lasso.\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [*] Friedman, Hastie, Tibshirani (2008).  Regularization paths for\n",
      "   generalized linear models via coordinate descent.  Journal of\n",
      "   Statistical Software 33(1), 1-22 Feb 2010.\n",
      "\n",
      ".. [*] A Belloni, V Chernozhukov, L Wang (2011).  Square-root Lasso:\n",
      "   pivotal recovery of sparse signals via conic programming.\n",
      "   Biometrika 98(4), 791-806. https://arxiv.org/pdf/1009.5689.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "print(sm.OLS.fit_regularized.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eacf2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statsmodels-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
